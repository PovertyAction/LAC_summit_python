{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAC Research and Data Summit\n",
    "## Natural Language Processing Lab\n",
    "### Isabel OÃ±ate - isabel.onate@northwestern.edu\n",
    "### 8/28/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Description: </span> \n",
    "\n",
    "NLP (Natural Language Processing) is a set of techniques for approaching text problems. In this lab we will go over an example of how to use text data to make predictions using a simple bag of words model. \n",
    "\n",
    "We will perform <a href=\"https://en.wikipedia.org/wiki/Sentiment_analysis\">sentiment analysis</a>, to identify and extract subjective information from movie reviews and categorize them inot positive and negative reviews.\n",
    "\n",
    "#### <span style=\"color:#a50e3e;\">Data: </span> \n",
    "\n",
    "The labeled data set consists of 50,000 IMDB movie reviews, selected for sentiment analysis. The sentiment of reviews is binary, 1 for positive reviews and 0 for negative reviews. No individual movie has more than 30 reviews. The data contains information on individual ids for each review (\"id\"), the sentiment of the review (\"sentiment\"), and the text with the review (\"review\").\n",
    "\n",
    "*_This exercise is based on this <a href=\"https://www.kaggle.com/c/word2vec-nlp-tutorial/overview\">kaggle tutorial</a> and modified to fit the objectives of the session._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np # numpy\n",
    "import pandas as pd # pandas\n",
    "from bs4 import BeautifulSoup # package for pulling data out of HTML and XML files\n",
    "import re # package for regular expresions\n",
    "import nltk #\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from sklearn.feature_extraction.text import CountVectorizer #\n",
    "from sklearn.ensemble import RandomForestClassifier # \n",
    "from sklearn.model_selection import train_test_split #\n",
    "import matplotlib.pyplot as plt #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = pd.read_csv('https://www.dropbox.com/s/qk7gc7ek68z5o7k/labeledData.tsv?dl=1', header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# \"header=0\" indicates that the first line of the file contains column names, \n",
    "# \"delimiter=\\t\" indicates that the fields are separated by tabs, \n",
    "# and quoting=3 for ignoring doubled quotes - to avoid errors when reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of data frame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'sentiment', 'review'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns in data frame\n",
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First observations in data frame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the first movie review\n",
    "print(data[\"review\"][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 4)\n",
      "(5000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test datasets for testing ML model\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=58934)\n",
    "train = train.reset_index() # reset the index of data frame\n",
    "test = test.reset_index() # reset the index of data frame\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First observations in train data frame\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">How do we clean the text data? </span> \n",
    "\n",
    "The cleaning depends on the task we are performing on the data. For this exercise we will:\n",
    "\n",
    "- 1) Remove markup \n",
    "- 2) Remove punctuation marks, numbers and other non-letter characters.\n",
    "- 3) Make everything lower case\n",
    "- 4) Remove stop words\n",
    "\n",
    "In some cases punctuation marks like \"?\" and \"!!!\" might be important. For this exercise we remove these in the interest of simplicity.\n",
    "\n",
    "We will also remove stop words. These are words that do not carry much meaning and that occur very frequently in language. For example \"the\", \"a\", \"and\". There are packages in python that contain lists of stop words in English. For this exercise we will use Python's <a href=\"http://www.nltk.org/\">Natural Language Toolkit (NLKT)</a>.\n",
    "\n",
    "We will use the <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Beautiful Soup package</a>, a library for pulling data out of HTML and XML files.\n",
    "\n",
    "There are many other things we could do to the data. For example, <a href=\"https://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\">stemming and lemmatizing</a> (both available in the NLTK package). The goal of both stemming and lemmatizing is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. This would allow us to group words that have a common base form like \"messages\", \"message\", and \"messaging\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_raw = \"This is an example of text containing numbers (like 2 3 4 5); \\\n",
    "non letters (# $ % ^ & *); some words with capital letters (Isabel Juan Ana); \\\n",
    "some stopwords (the is on a); and markup (<br /><br />).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of text containing numbers (like 2 3 4 5); non letters (# $ % ^ & *); some words with capital letters (Isabel Juan Ana); some stopwords (the is on a); and markup (<br /><br />).\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look\n",
    "print(example_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of text containing numbers (like 2 3 4 5); non letters (# $ % ^ & *); some words with capital letters (Isabel Juan Ana); some stopwords (the is on a); and markup ().\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BeautifulSoup object\n",
    "example = BeautifulSoup(train[\"review\"][0]) \n",
    "example = BeautifulSoup(example_raw) \n",
    "\n",
    "# Extract text\n",
    "example_text = example.get_text() # function to get the text in the review without tags or markup\n",
    "print(example_text) # See how the markup was removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of text containing numbers  like           non letters                some words with capital letters  Isabel Juan Ana   some stopwords  the is on a   and markup    \n"
     ]
    }
   ],
   "source": [
    "# Remove non-letters\n",
    "letters_only = re.sub(\"[^a-zA-Z]\", \" \", example_text) \n",
    "print(letters_only) # See how numbers and symbols were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example of text containing numbers  like           non letters                some words with capital letters  isabel juan ana   some stopwords  the is on a   and markup    \n"
     ]
    }
   ],
   "source": [
    "# Make everythong lower case\n",
    "print(letters_only.lower()) # See how everything is lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'example', 'of', 'text', 'containing', 'numbers', 'like', 'non', 'letters', 'some', 'words', 'with', 'capital', 'letters', 'isabel', 'juan', 'ana', 'some', 'stopwords', 'the', 'is', 'on', 'a', 'and', 'markup']\n"
     ]
    }
   ],
   "source": [
    "# Split into vector of words\n",
    "words = letters_only.lower().split() \n",
    "# Print the first 6 words in the vector\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the stop words in nltk package \n",
    "print(stopwords.words(\"english\")[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'text', 'containing', 'numbers', 'like', 'non', 'letters', 'words', 'capital', 'letters', 'isabel', 'juan', 'ana', 'stopwords', 'markup']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "meaning_words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "print(meaning_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare text data for analysis\n",
    "# -input: one observation of raw text data\n",
    "# -output: one observation of raw clean text data\n",
    "def prepare_words(rawtext): \n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(rawtext).get_text() \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    # 4. Convert the stop words to a set for efficiency\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    # 6. Return the union of the words\n",
    "    return( \" \".join( meaningful_words ))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I think that if I went to a first school somewhere deep in the countryside and asked the bottom set of English to come up with a script, it would make more sense than this. I could then go to the first year drama group and they would act it out better than the jokers in this film. This sounds really mean, but I'm certain that they made this as a joke and are entirely aware that they possess (see what I did there?) neither the skills to act or to write anything, ever.<br /><br />Watch this only if you're incredibly drunk, high or in need of a good excuse as to why your decaying corpse was found with slit wrists. I will now go to my fish bowl and collect all of the poo at the bottom. After that, I will mould it into the shape of a disc and put it into my DVD player, fully expecting it to produce something far better than this trumpery.<br /><br />Acting - 0/10 Plot - LOL/10 Breasts - 9/10\"\n"
     ]
    }
   ],
   "source": [
    "# lets look at the first review in the train data frame\n",
    "print(train[\"review\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "think went first school somewhere deep countryside asked bottom set english come script would make sense could go first year drama group would act better jokers film sounds really mean certain made joke entirely aware possess see neither skills act write anything ever watch incredibly drunk high need good excuse decaying corpse found slit wrists go fish bowl collect poo bottom mould shape disc put dvd player fully expecting produce something far better trumpery acting plot lol breasts\n"
     ]
    }
   ],
   "source": [
    "# Cean first review\n",
    "clean_review_1 = prepare_words(train[\"review\"][0])\n",
    "# View clean form\n",
    "print(clean_review_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of reviews in dataframe\n",
    "num_reviews = train[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review and clean it\n",
    "for i in range(0, num_reviews): \n",
    "    # Call our function for each one, and add the result to the list of clean reviews\n",
    "    clean_train_reviews.append(prepare_words(train[\"review\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What type of obejct is it?\n",
    "type(clean_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think went first school somewhere deep countryside asked bottom set english come script would make sense could go first year drama group would act better jokers film sounds really mean certain made joke entirely aware possess see neither skills act write anything ever watch incredibly drunk high need good excuse decaying corpse found slit wrists go fish bowl collect poo bottom mould shape disc put dvd player fully expecting produce something far better trumpery acting plot lol breasts',\n",
       " 'never big fan television watched first time got series late season ended even saw first episode episode series parents dvr digital video recorder box house sitting weekend took one episode hook line sinker world jack bauer boy hooked watched next six episodes without blinking eye next day went blockbuster signed unlimited month pass twenty something dollars needless say greatest blockbuster money ever spent watched first three seasons three weeks forty minute episodes say finding happens next easier dvd waiting entire week imagine anticipation watching season week week find mildly torturous cruel going give try watch like rest america dvr set bet chomping bit']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Going from text to numbers... </span> \n",
    "\n",
    "After cleaning the text data, we need to convert it into a numerical representaion for the ML algorithm. We will use a <a href=\"https://en.wikipedia.org/wiki/Bag-of-words_model\">Bag of words</a> model which counts the number of times a word appears in each entry. We will use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html\">feature_extraction</a> module from scikit-learn to create bag-of-words features.\n",
    "\n",
    "Lets go over an example of how this works. Suppose we are working with 2 text entries:\n",
    "\n",
    "- Sentence 1: \"The cat sat on the hat\"\n",
    "- Sentence 2: \"The dog ate the cat and the hat\"\n",
    "\n",
    "From these two sentences, our vocabulary space is as follows:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "To get our bags of words, we count the number of times each word occurs in each sentence. In Sentence 1, \"the\" appears twice, and \"cat\", \"sat\", \"on\", and \"hat\" each appear once, so the feature vector for Sentence 1 is:\n",
    "\n",
    "{ 2, 1, 1, 1, 1, 0, 0, 0 }\n",
    "\n",
    "Similarly, the features for Sentence 2 are: \n",
    "\n",
    "{ 3, 1, 0, 0, 1, 1, 1, 1}\n",
    "\n",
    "In the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 1,000 most frequent words (remembering that stop words have already been removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object -scikit-learn's bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 1000) # we use the 5000 most common words\n",
    "\n",
    "# Fit the model and lean vocabulary; then transform data into feature vectors\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews) # imput needs to be a list of strings\n",
    "\n",
    "# Convert list into numpy array for ML model\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(20000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Type of object\n",
    "print(type(train_data_features))\n",
    "# Chape of vector\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'absolutely', 'across', 'act', 'acted', 'acting', 'action', 'actor', 'actors', 'actress']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Supervised Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a numeric representation of the train data. We can do supervised learning to predict sentiment labels! \n",
    "\n",
    "We will start with a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">random forest classifier</a> included in the scikit-learn package. Random forests is a supervised learning algorithm that can be used for both classification and regression problems. \n",
    "\n",
    "We will fit the model to tha data using our train dataset and then test it on our train subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100, random_state = 736438) # more trees is better but will take longer to run\n",
    "\n",
    "# Fit the forest to the training data\n",
    "forest = forest.fit(train_data_features, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to prepare the test data just like we did with the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list and append the clean reviews\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    #clean_review = prepare_words(test[\"review\"][i] )\n",
    "    clean_test_reviews.append(prepare_words(test[\"review\"][i]))\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.38 %\n"
     ]
    }
   ],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Look at accuracy of the model at predicting sentiment\n",
    "correct = np.array(test[\"sentiment\"] == result) # Count number of accurate predictions\n",
    "accuracy = (np.sum(correct)/test[\"sentiment\"].size)*100 # Percent accurate\n",
    "print(\"Accuracy:\", round(accuracy,2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the results to the test dataframe\n",
    "test['prediction'] = result\n",
    "\n",
    "# Export into a comma-separated output file\n",
    "test.to_csv(\"Bag_of_Words_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23250</td>\n",
       "      <td>\"6225_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I doubt this will ever even be a cult film. I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3873</td>\n",
       "      <td>\"11465_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This film is scary because you can find yours...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2456</td>\n",
       "      <td>\"8879_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I think Cliff Robertson certainly was one of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9054</td>\n",
       "      <td>\"12115_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I've been working my way through a collection...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3924</td>\n",
       "      <td>\"2945_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I watched this movie when Joe Bob Briggs host...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20438</td>\n",
       "      <td>\"1304_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Okay. Yes, this was a very-tight-budget movie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23208</td>\n",
       "      <td>\"611_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I contend that whoever is ultimately responsi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6364</td>\n",
       "      <td>\"8878_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This movie had lots of great actors and actre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5723</td>\n",
       "      <td>\"11254_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\\"Sky Captain and the World of Tomorrow\\\" (an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6944</td>\n",
       "      <td>\"4734_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"You like beautiful girls? Yeah me too. What i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         id  sentiment  \\\n",
       "0  23250   \"6225_1\"          0   \n",
       "1   3873  \"11465_7\"          1   \n",
       "2   2456   \"8879_4\"          0   \n",
       "3   9054  \"12115_4\"          0   \n",
       "4   3924   \"2945_1\"          0   \n",
       "5  20438   \"1304_7\"          1   \n",
       "6  23208   \"611_10\"          1   \n",
       "7   6364   \"8878_9\"          1   \n",
       "8   5723  \"11254_1\"          0   \n",
       "9   6944   \"4734_2\"          0   \n",
       "\n",
       "                                              review  prediction  \n",
       "0  \"I doubt this will ever even be a cult film. I...           0  \n",
       "1  \"This film is scary because you can find yours...           1  \n",
       "2  \"I think Cliff Robertson certainly was one of ...           0  \n",
       "3  \"I've been working my way through a collection...           1  \n",
       "4  \"I watched this movie when Joe Bob Briggs host...           0  \n",
       "5  \"Okay. Yes, this was a very-tight-budget movie...           1  \n",
       "6  \"I contend that whoever is ultimately responsi...           1  \n",
       "7  \"This movie had lots of great actors and actre...           1  \n",
       "8  \"\\\"Sky Captain and the World of Tomorrow\\\" (an...           0  \n",
       "9  \"You like beautiful girls? Yeah me too. What i...           0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final output\n",
    "test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
