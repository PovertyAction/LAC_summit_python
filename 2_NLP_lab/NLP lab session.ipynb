{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAC Data and Research Summit\n",
    "## Natural Language Processing Lab\n",
    "### Isabel Oñate - isabel.onate@northwestern.edu\n",
    "### 8/28/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Description: </span> \n",
    "\n",
    "NLP (Natural Language Processing) is a set of techniques for approaching text problems. In this lab we will go over an example of how to use text data to make predictions using a simple bag of words model. \n",
    "\n",
    "We will perform <a href=\"https://en.wikipedia.org/wiki/Sentiment_analysis\">sentiment analysis</a>, to identify and extract subjective information from movie reviews and categorize them inot positive and negative reviews.\n",
    "\n",
    "#### <span style=\"color:#a50e3e;\">Data: </span> \n",
    "\n",
    "The labeled data set consists of 50,000 IMDB movie reviews, selected for sentiment analysis. The sentiment of reviews is binary, 1 for positive reviews and 0 for negative reviews. No individual movie has more than 30 reviews. The data contains information on individual ids for each review (\"id\"), the sentiment of the review (\"sentiment\"), and the text with the review (\"review\").\n",
    "\n",
    "*_This exercise is based on this <a href=\"https://www.kaggle.com/c/word2vec-nlp-tutorial/overview\">kaggle tutorial</a> and modified to fit the objectives of the session._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np # numpy\n",
    "import pandas as pd # pandas\n",
    "from bs4 import BeautifulSoup # package for pulling data out of HTML and XML files\n",
    "import re # package for regular expresions\n",
    "import nltk #\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from sklearn.feature_extraction.text import CountVectorizer #\n",
    "from sklearn.ensemble import RandomForestClassifier # \n",
    "from sklearn.model_selection import train_test_split #\n",
    "import matplotlib.pyplot as plt #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = pd.read_csv('https://www.dropbox.com/s/qk7gc7ek68z5o7k/labeledData.tsv?dl=1', header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# \"header=0\" indicates that the first line of the file contains column names, \n",
    "# \"delimiter=\\t\" indicates that the fields are separated by tabs, \n",
    "# and quoting=3 for ignoring doubled quotes - to avoid errors when reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of data frame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'sentiment', 'review'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns in data frame\n",
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First observations in data frame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the first movie review\n",
    "print(data[\"review\"][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 4)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test datasets for testing ML model\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=58934)\n",
    "train = train.reset_index() # reset the index of data frame\n",
    "test = test.reset_index() # reset the index of data frame\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>\"7369_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I had a feeling that after \\\"Submerged\\\", thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>\"11744_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"Mr. Harvey Lights a Candle\\\" is anchored by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         id  sentiment  \\\n",
       "0      1   \"2381_9\"          1   \n",
       "1      2   \"7759_3\"          0   \n",
       "2     13   \"7369_1\"          0   \n",
       "3     12  \"11744_9\"          1   \n",
       "4      0   \"5814_8\"          1   \n",
       "\n",
       "                                              review  \n",
       "0  \"\\\"The Classic War of the Worlds\\\" by Timothy ...  \n",
       "1  \"The film starts with a manager (Nicholas Bell...  \n",
       "2  \"I had a feeling that after \\\"Submerged\\\", thi...  \n",
       "3  \"\\\"Mr. Harvey Lights a Candle\\\" is anchored by...  \n",
       "4  \"With all this stuff going down at the moment ...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First observations in train data frame\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">How do we clean the text data? </span> \n",
    "\n",
    "The cleaning depends on the task we are performing on the data. For this exercise we will:\n",
    "\n",
    "- 1) Remove markup \n",
    "- 2) Remove punctuation marks, numbers and other non-letter characters.\n",
    "- 3) Make everything lower case\n",
    "- 4) Remove stop words\n",
    "\n",
    "In some cases punctuation marks like \"?\" and \"!!!\" might be important. For this exercise we remove these in the interest of simplicity.\n",
    "\n",
    "We will also remove stop words. These are words that do not carry much meaning and that occur very frequently in language. For example \"the\", \"a\", \"and\". There are packages in python that contain lists of stop words in English. For this exercise we will use Python's <a href=\"http://www.nltk.org/\">Natural Language Toolkit (NLKT)</a>.\n",
    "\n",
    "It is not considered a reliable practice to remove markup using regular expressions so we will use the <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">Beautiful Soup package</a>, a library for pulling data out of HTML and XML files.\n",
    "\n",
    "There are many other things we could do to the data. For example, <a href=\"https://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\">stemming and lemmatizing</a> (both available in the NLTK package). The goal of both stemming and lemmatizing is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. This would allow us to group words that have a common base form like \"messages\", \"message\", and \"messaging\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_raw = \"This is an example of text containing numbers (like 2 3 4 5); \\\n",
    "non letters (# $ % ^ & *); some words with capital letters (Isabel Juan Ana); \\\n",
    "some stopwords (the is on a); and markup (<br /><br />).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of text containing numbers (like 2 3 4 5); non letters (# $ % ^ & *); some words with capital letters (Isabel Juan Ana); some stopwords (the is on a); and markup (<br /><br />).\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look\n",
    "print(example_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of text containing numbers (like 2 3 4 5); non letters (# $ % ^ & *); some words with capital letters (Isabel Juan Ana); some stopwords (the is on a); and markup ().\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BeautifulSoup object\n",
    "example = BeautifulSoup(train[\"review\"][0]) ## what is this doing\n",
    "example = BeautifulSoup(example_raw) ## what is this doing\n",
    "# Extract text\n",
    "example_text = example.get_text() # function to get the text in the review without tags or markup\n",
    "print(example_text) # See how the markup was removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of text containing numbers  like           non letters                some words with capital letters  Isabel Juan Ana   some stopwords  the is on a   and markup    \n"
     ]
    }
   ],
   "source": [
    "# Remove non-letters\n",
    "letters_only = re.sub(\"[^a-zA-Z]\", \" \", example_text) \n",
    "print(letters_only) # See how numbers and symbols were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example of text containing numbers  like           non letters                some words with capital letters  isabel juan ana   some stopwords  the is on a   and markup    \n"
     ]
    }
   ],
   "source": [
    "# Make everythong lower case\n",
    "print(letters_only.lower()) # See how everything is lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'example', 'of']\n"
     ]
    }
   ],
   "source": [
    "# Split into vector of words\n",
    "words = letters_only.lower().split() \n",
    "# Print the first 6 words in the vector\n",
    "print(words[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the stop words in nltk package \n",
    "print(stopwords.words(\"english\")[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'text', 'containing', 'numbers', 'like', 'non', 'letters', 'words', 'capital', 'letters', 'isabel', 'juan', 'ana', 'stopwords', 'markup']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "meaning_words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "print(meaning_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare text data for analysis\n",
    "# -input: one observation of raw text data\n",
    "# -output: one observation of raw clean text data\n",
    "def prepare_words(rawtext): \n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(rawtext).get_text() \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    # 4. Convert the stop words to a set for efficiency\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    # 6. Return the union of the words\n",
    "    return( \" \".join( meaningful_words ))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\\"The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\"critics\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\"critics\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\"critics\\\" perceive to be its shortcomings.\"\n"
     ]
    }
   ],
   "source": [
    "# lets look at the first review in the train data frame\n",
    "print(train[\"review\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classic war worlds timothy hines entertaining film obviously goes great effort lengths faithfully recreate h g wells classic book mr hines succeeds watched film appreciated fact standard predictable hollywood fare comes every year e g spielberg version tom cruise slightest resemblance book obviously everyone looks different things movie envision amateur critics look criticize everything others rate movie important bases like entertained people never agree critics enjoyed effort mr hines put faithful h g wells classic novel found entertaining made easy overlook critics perceive shortcomings\n"
     ]
    }
   ],
   "source": [
    "# Cean first review\n",
    "clean_review_1 = prepare_words(train[\"review\"][0])\n",
    "# View clean form\n",
    "print(clean_review_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of reviews in dataframe\n",
    "num_reviews = train[\"review\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "# Loop over each review and clean it\n",
    "for i in range(0, num_reviews): \n",
    "    # Call our function for each one, and add the result to the list of clean reviews\n",
    "    clean_train_reviews.append(prepare_words(train[\"review\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What type of obejct is it?\n",
    "type(clean_train_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Going from text to numbers... </span> \n",
    "\n",
    "After cleaning the text data, we need to convert it into a numerical representaion for the ML algorithm. We will use a <a href=\"https://en.wikipedia.org/wiki/Bag-of-words_model\">Bag of words</a> model which counts the number of times a word appears in each entry. We will use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html\">feature_extraction</a> module from scikit-learn to create bag-of-words features.\n",
    "\n",
    "Lets go over an example of how this works. Suppose we are working with 2 text entries:\n",
    "\n",
    "- Sentence 1: \"The cat sat on the hat\"\n",
    "- Sentence 2: \"The dog ate the cat and the hat\"\n",
    "\n",
    "From these two sentences, our vocabulary space is as follows:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "To get our bags of words, we count the number of times each word occurs in each sentence. In Sentence 1, \"the\" appears twice, and \"cat\", \"sat\", \"on\", and \"hat\" each appear once, so the feature vector for Sentence 1 is:\n",
    "\n",
    "{ 2, 1, 1, 1, 1, 0, 0, 0 }\n",
    "\n",
    "Similarly, the features for Sentence 2 are: \n",
    "\n",
    "{ 3, 1, 0, 0, 1, 1, 1, 1}\n",
    "\n",
    "In the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 5000 most frequent words (remembering that stop words have already been removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object -scikit-learn's bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000) # we use the 5000 most common words\n",
    "\n",
    "# Fit the model and lean vocabulary; then transform data into feature vectors\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews) # imput needs to be a list of strings\n",
    "\n",
    "# Convert list into numpy array for ML model\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(11, 892)\n"
     ]
    }
   ],
   "source": [
    "# Type of object\n",
    "print(type(train_data_features))\n",
    "# Chape of vector\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['action', 'actors', 'acts', 'actual', 'actually', 'addition', 'adds', 'afoul', 'agent', 'agree']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Supervised Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a numeric representation of the train data. We can do supervised learning to predict sentiment labels! \n",
    "\n",
    "We will start with a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">random forest classifier</a> included in the scikit-learn package. Random forests is a supervised learning algorithm that can be used for both classification and regression problems. \n",
    "\n",
    "We will fit the model to tha data using our train dataset and then test it on our train subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100, random_state = 736438) # more trees is better but will take longer to run\n",
    "\n",
    "# Fit the forest to the training data\n",
    "forest = forest.fit(train_data_features, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to prepare the test data just like we did with the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list and append the clean reviews\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    #clean_review = prepare_words(test[\"review\"][i] )\n",
    "    clean_test_reviews.append(prepare_words(test[\"review\"][i]))\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.67 %\n"
     ]
    }
   ],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Look at accuracy of the model at predicting sentiment\n",
    "correct = np.array(test[\"sentiment\"] == result) # Count number of accurate predictions\n",
    "accuracy = (np.sum(correct)/test[\"sentiment\"].size)*100 # Percent accurate\n",
    "print(\"Accuracy:\", round(accuracy,2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the results to the test dataframe\n",
    "test['prediction'] = result\n",
    "\n",
    "# Export into a comma-separated output file\n",
    "test.to_csv(\"Bag_of_Words_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>\"7166_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"This movie could have been very good, but com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>\"319_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"A friend of mine bought this film for £1, and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        id  sentiment  \\\n",
       "0      6  \"7166_2\"          0   \n",
       "1      8   \"319_1\"          0   \n",
       "2      4  \"9495_8\"          1   \n",
       "\n",
       "                                              review  prediction  \n",
       "0  \"This movie could have been very good, but com...           1  \n",
       "1  \"A friend of mine bought this film for £1, and...           0  \n",
       "2  \"Superbly trashy and wondrously unpretentious ...           1  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final output\n",
    "test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.sum(train_data_features, axis=0)\n",
    "words = []\n",
    "for tag, count in zip(vectorizer.get_feature_names(), dist):\n",
    "    words.append(tag)  \n",
    "words = np.array(words)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = pd.DataFrame(data={\"word\":words, \"frequency\":dist})\n",
    "common_words = common_words[common_words['frequency']>10000]\n",
    "common_words.index = range(len(common_words.index)) # redefine index of data frame \n",
    "common_words = common_words.sort_values(by=['frequency'])\n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abandoned', 'abc', 'abilities', ..., 'zombie', 'zombies', 'zone'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot\n",
    "pos = np.arange(len(common_words.frequency))\n",
    "width = 1.0\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(pos + (width / 2))\n",
    "ax.set_xticklabels(common_words.word)\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(\"Most common words\")\n",
    "plot = plt.bar(pos, common_words.frequency, width, color = \"light blue\", edgecolor='black')\n",
    "plt.xticks(rotation=90)\n",
    "plt.rcParams[\"figure.figsize\"] = [40,20]\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
